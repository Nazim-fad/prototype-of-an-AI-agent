llm:
  model: "qwen2.5:0.5b"  # it is an instruct model and it is small enough to run locally on my machine
  request_timeout: 120.0

chat_agent:
  model_id: "meta-llama/Meta-Llama-3.1-70B-Instruct"
  temperature: 0.1
